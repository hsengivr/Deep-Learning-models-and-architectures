{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRz0ELPP/T1rj4T3WtRswp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":59,"metadata":{"id":"wpq1StKqqXBC","executionInfo":{"status":"ok","timestamp":1708087191798,"user_tz":-330,"elapsed":401,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from math import ceil"]},{"cell_type":"code","source":["base_model = [\n","    #expand_ratio, channels, repeats/layers, stride, kernel_size\n","    [1, 16, 1, 1, 3],\n","    [6, 24, 2, 2, 3],\n","    [6, 40, 2, 2, 5],\n","    [6, 80, 3, 2, 3],\n","    [6, 112, 3, 1, 5],\n","    [6, 192, 4, 2, 5],\n","    [6, 320, 1, 1, 3]]\n","\n"],"metadata":{"id":"C_rQ66_kqeG1","executionInfo":{"status":"ok","timestamp":1708087192246,"user_tz":-330,"elapsed":17,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["phi_values = {\n","    # tuple( phi_value, resolution, drop_rate)\n","    \"b0\" : (0, 224, 0.2), # alpha, beta, gamma, depth = alpha**phi\n","    \"b1\" : (0.5, 240, 0.2),\n","    \"b2\" : (1, 260, 0.3),\n","    \"b3\" : (2, 300, 0.3),\n","    \"b4\" : (3, 380, 0.4),\n","    \"b5\" : (4, 456, 0.4),\n","    \"b6\" : (5, 528, 0.5),\n","    \"b7\" : (6, 608, 0.5),\n","    }"],"metadata":{"id":"5cLEGM7arXdI","executionInfo":{"status":"ok","timestamp":1708087192246,"user_tz":-330,"elapsed":14,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["class CNNBlock(nn.Module):\n","  \"\"\"\n","  groups =1 normal conv, groups = in_channels = depthwise conv\n","  In normal conv of 3x3 kernel it would be a cube spanning across HxWxC (3X3x3)\n","  But for depthwise conv we need to apply 3x3 kernel for each channel/filter independently form of 3x3x1\n","  \"\"\"\n","\n","  def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\n","    super(CNNBlock, self).__init__()\n","    self.cnn = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups,bias=False)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.silu = nn.SiLU() # silu <-> swish\n","\n","  def forward(self, x):\n","    x = self.cnn(x)\n","    x = self.bn(x)\n","    x = self.silu(x)\n","    return x\n"],"metadata":{"id":"-30MquLjsbAW","executionInfo":{"status":"ok","timestamp":1708087192246,"user_tz":-330,"elapsed":12,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["class SqueezeExcitation(nn.Module):\n","  \"\"\"\n","  Increase channel wise attention\n","  Adaptive pool - takes HxW and gives one value as output\n","\n","  output each channel values between 0 and 1\n","  \"\"\"\n","  def __init__(self, in_channels, reduce_dim):\n","    super(SqueezeExcitation, self).__init__()\n","    self.se = nn.Sequential(\n","        nn.AdaptiveAvgPool2d(1), # cxhxw - cx1x1\n","        nn.Conv2d(in_channels, reduce_dim, 1),\n","        nn.SiLU(),\n","        nn.Conv2d(reduce_dim, in_channels, 1),\n","        nn.Sigmoid()\n","    )\n","\n","  def forward(self, x):\n","    return x*self.se(x)"],"metadata":{"id":"GOesduo_sl0c","executionInfo":{"status":"ok","timestamp":1708087192247,"user_tz":-330,"elapsed":12,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["class InvertedResidualBlock(nn.Module):\n","  def __init__(self,\n","               in_channels,\n","               out_channels,\n","               kernel_size,\n","               stride,\n","               padding,\n","               expand_ratio, # depthwise conv2d\n","               reduction=4,  # squeeze excitation\n","               survival_prob = 0.8 # for stochastic depth\n","               ):\n","\n","    super(InvertedResidualBlock, self).__init__()\n","    self.survival_prob = 0.8\n","    # residual wont be applied when downsampling\n","    self.use_residual = in_channels == out_channels and stride == 1\n","    hidden_dim = in_channels * expand_ratio\n","    self.expand = in_channels != hidden_dim\n","    reduce_dim = int(in_channels/reduction)\n","\n","    if self.expand:\n","      self.expand_conv = CNNBlock(\n","          in_channels, hidden_dim, kernel_size=3, stride=1, padding=1\n","      )\n","\n","    self.conv = nn.Sequential(\n","        CNNBlock(hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim),\n","        SqueezeExcitation(hidden_dim, reduce_dim),\n","        nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n","        nn.BatchNorm2d(out_channels)\n","    )\n","\n","  def stochastic_depth(self, x):\n","    # type of dropout\n","    if not self.training:\n","      return x\n","\n","    binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n","    return torch.div(x, self.survival_prob) * binary_tensor\n","\n","\n","  def forward(self, inputs):\n","    x = self.expand_conv(inputs) if self.expand else inputs\n","\n","    if self.use_residual:\n","      return self.stochastic_depth(self.conv(x)) + inputs\n","    else:\n","      return self.conv(x)\n",""],"metadata":{"id":"jaVkNRyusuG0","executionInfo":{"status":"ok","timestamp":1708087192247,"user_tz":-330,"elapsed":10,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["class EfficientNet(nn.Module):\n","  def __init__(self, version, num_classes):\n","    super(EfficientNet, self).__init__()\n","    width_factor, depth_factor, dropout_rate = self.calculate_factors(version)\n","    last_channels = ceil(1280 * width_factor)\n","    self.pool = nn.AdaptiveAvgPool2d(1)\n","    self.features = self.create_features(width_factor, depth_factor, last_channels)\n","    self.classifier = nn.Sequential(\n","        nn.Dropout(dropout_rate),\n","        nn.Linear(last_channels, num_classes)\n","    )\n","\n","  def calculate_factors(self, version, alpha=1.2, beta=1.1):\n","    phi, res, drop_rate = phi_values[version]\n","    depth_factor = alpha**phi\n","    width_factor = beta ** phi\n","    return depth_factor, width_factor, drop_rate\n","\n","\n","  def create_features(self, width_factor, depth_factor, last_channels):\n","    channels = int(32 * width_factor)\n","    features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n","    in_channels = channels\n","\n","    for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n","      out_channels = 4*ceil(int(channels*width_factor)/4)\n","      layer_repeats = ceil(repeats*depth_factor)\n","\n","      for layer in range(layer_repeats):\n","        features.append(\n","            InvertedResidualBlock(\n","                in_channels,\n","                out_channels,\n","                expand_ratio = expand_ratio,\n","                stride = stride if layer==0 else 1,\n","                kernel_size = kernel_size,\n","                padding = kernel_size//2 , #if k=1:pad=0, k=3:pad=1, k=5:pad=2\n","            )\n","        )\n","\n","        in_channels = out_channels\n","    features.append(\n","        CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)\n","    )\n","\n","    return nn.Sequential(*features)\n","\n","  def forward(self, x):\n","    x = self.pool(self.features(x))\n","    return self.classifier(x.view(x.shape[0], -1))"],"metadata":{"id":"l_y4J9eisxsb","executionInfo":{"status":"ok","timestamp":1708087192247,"user_tz":-330,"elapsed":9,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["def test():\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  version = 'b0'\n","  phi, res, drop_rate = phi_values[version]\n","  num_examples, num_classes = 4, 10\n","  x =torch.randn((num_examples, 3, res, res)).to(device)\n","  model = EfficientNet(\n","      version=version,\n","      num_classes = num_classes\n","  ).to(device)\n","\n","  print(model(x).shape) # (num_examples, num_classes)\n"],"metadata":{"id":"2RDLLd4Qs3Lk","executionInfo":{"status":"ok","timestamp":1708087256509,"user_tz":-330,"elapsed":564,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xn1D4KLz5EvG","executionInfo":{"status":"ok","timestamp":1708087257667,"user_tz":-330,"elapsed":1162,"user":{"displayName":"vignesh R","userId":"01030958218173470676"}},"outputId":"85c30e8a-7431-441f-944f-d612cc13aed4"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 10])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"sNn4-wGv7qtW"},"execution_count":null,"outputs":[]}]}